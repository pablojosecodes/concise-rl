{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb78e073",
   "metadata": {},
   "source": [
    "Value based funcitons are based on the Bellman Equations.\n",
    "- $V^*(s) = \\underset{a}{\\text{max}} \\sum_{s'} P(s'|s,a)(R(s,a) + \\gamma V^*(s'))$\n",
    "- $Q^*(s,a) =  \\sum_{s'} P(s'|s,a)[R(s,a) + \\gamma \\underset{a}{\\text{max}} Q^*(s'])$\n",
    "\n",
    "And for completion- the deterministic versions\n",
    "- $V^*(s) = \\underset{a}{\\text{max}}(R(s,a) + \\gamma\\ V^*(s’))$\n",
    "- $Q^*(s,a) = R(s,a)  + \\gamma\\  \\underset{a'}{\\text{max}}\\ Q^*(s’,a'))$\n",
    "\n",
    "The goal is to optimize for expected reward which is defined as follows\n",
    "- $R = E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_t R(s_t,a_t)]$\n",
    "\n",
    "If we know the transitions for all states (and there's a small number of them), we can just do policy iteration:\n",
    "- Evaluate best possible next action given state and current value function, take that action\n",
    "- Update value function based on observed reward\n",
    "\n",
    "Since we know all the transitions and states, we can essentialy just fill out a table of the values we know\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99cbc5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The initial random policy is:\n",
      "\n",
      "| Left  | Down  | Up    | +1    |\n",
      "| Right | WALL  | Up    | -1    |\n",
      "| Down  | Right | Up    | Left  |\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Down  | WALL  | Left  | -1    |\n",
      "| Right | Right | Up    | Down  |\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Up    | WALL  | Left  | -1    |\n",
      "| Right | Right | Up    | Down  |\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Up    | WALL  | Left  | -1    |\n",
      "| Up    | Right | Up    | Down  |\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Up    | WALL  | Left  | -1    |\n",
      "| Up    | Left  | Up    | Down  |\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Up    | WALL  | Left  | -1    |\n",
      "| Up    | Left  | Left  | Down  |\n",
      "\n",
      "The optimal policy is:\n",
      "\n",
      "| Right | Right | Right | +1    |\n",
      "| Up    | WALL  | Left  | -1    |\n",
      "| Up    | Left  | Left  | Down  |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "decay = 0.99\n",
    "maximum = 10**(-3)\n",
    "\n",
    "action_len = 4\n",
    "actions = [(1, 0), (0, -1), (-1, 0), (0, 1)] # Down, Left, Up, Right\n",
    "row_len = 3\n",
    "col_len = 4\n",
    "utility = [[0, 0, 0, 1], [0, 0, 0, -1], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "policy = [[random.randint(0, 3) for j in range(col_len)] for i in range(row_len)] # construct a random policy\n",
    "\n",
    "# Visualization\n",
    "def printEnvironment(arr, policy=False):\n",
    "    res = \"\"\n",
    "    for r in range(row_len):\n",
    "        res += \"|\"\n",
    "        for c in range(col_len):\n",
    "            if r == c == 1:\n",
    "                val = \"WALL\"\n",
    "            elif r <= 1 and c == 3:\n",
    "                val = \"+1\" if r == 0 else \"-1\"\n",
    "            else:\n",
    "                val = [\"Down\", \"Left\", \"Up\", \"Right\"][arr[r][c]]\n",
    "            res += \" \" + val[:5].ljust(5) + \" |\" # format\n",
    "        res += \"\\n\"\n",
    "    print(res)\n",
    "\n",
    "\n",
    "def action_utility(utility, r, c, action):\n",
    "    dr, dc = actions[action]\n",
    "    newR, newC = r+dr, c+dc\n",
    "    if newR < 0 or newC < 0 or newR >= row_len or newC >= col_len or (newR == newC == 1):\n",
    "        return utility[r][c]\n",
    "    else:\n",
    "        return utility[newR][newC]\n",
    "\n",
    "\n",
    "def get_utilities(utility, r, c, action):\n",
    "    u = 0\n",
    "    u += 0.1 * decay * action_utility(utility, r, c, (action-1)%4)\n",
    "    u += 0.8 * decay * action_utility(utility, r, c, action)\n",
    "    u += 0.1 * decay * action_utility(utility, r, c, (action+1)%4)\n",
    "    return u\n",
    "\n",
    "def policyEvaluation(policy, utility):\n",
    "    while True:\n",
    "        next_utility = [[0, 0, 0, 1], [0, 0, 0, -1], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
    "        error = 0\n",
    "        for r in range(row_len):\n",
    "            for c in range(col_len):\n",
    "                if (r <= 1 and c == 3) or (r == c == 1):\n",
    "                    continue\n",
    "                next_utility[r][c] = get_utilities(utility, r, c, policy[r][c]) \n",
    "                error = max(error, abs(next_utility[r][c]-utility[r][c]))\n",
    "        utility = next_utility\n",
    "        if error < maximum * (1-decay) / decay:\n",
    "            break\n",
    "    return utility\n",
    "\n",
    "def policyIteration(policy, utility):\n",
    "    while True:\n",
    "        utility = policyEvaluation(policy, utility)\n",
    "        unchanged = True\n",
    "        for r in range(row_len):\n",
    "            for c in range(col_len):\n",
    "                if (r <= 1 and c == 3) or (r == c == 1):\n",
    "                    continue\n",
    "                maxAction, max_util = None, -float(\"inf\")\n",
    "                for action in range(action_len):\n",
    "                    u = get_utilities(utility, r, c, action)\n",
    "                    if u > max_util:\n",
    "                        maxAction, max_util = action, u\n",
    "                if max_util > get_utilities(utility, r, c, policy[r][c]):\n",
    "                    policy[r][c] = maxAction # the action that maximizes the utility\n",
    "                    unchanged = False\n",
    "        if unchanged:\n",
    "            break\n",
    "        printEnvironment(policy)\n",
    "    return policy\n",
    "\n",
    "print(\"The initial random policy is:\\n\")\n",
    "printEnvironment(policy)\n",
    "# Policy iteration\n",
    "policy = policyIteration(policy, utility)\n",
    "# Print the optimal policy\n",
    "print(\"The optimal policy is:\\n\")\n",
    "printEnvironment(policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
