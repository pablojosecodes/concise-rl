{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774af478",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "While Q learning was all about value functoins and Bellman equations, policy gradients are all about (as you may have guessed) estimating policy gradients to iterate to the best possible policy.\n",
    "\n",
    "The derivation is quite fun but we know that $\\Delta_\\theta J(\\theta)$  = $= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}} \\left[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t|s_t)R(\\tau) \\right] \\quad \\text{Expression for grad-log-prob}$\n",
    "- The estimate we'll use is $\\hat{g} = \\frac{1}{|\\mathcal{D}|} \\sum_{\\tau \\in \\mathcal{D}} \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) R(\\tau)$\n",
    "\n",
    "**The REINFORCE algorithm** is quite simple really\n",
    "1. Generate sample trajectories \n",
    "2. Generate policy gradient\n",
    "3. Improve policy: $\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcfa9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make('Acrobot-v1', render_mode=\"human\")\n",
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a2e76e",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m observation, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# agent policy that uses the observation and info\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:75\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    The reset environment\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:61\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:59\u001b[0m, in \u001b[0;36mPassiveEnvChecker.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_reset_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/classic_control/acrobot.py:198\u001b[0m, in \u001b[0;36mAcrobotEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39muniform(low\u001b[38;5;241m=\u001b[39mlow, high\u001b[38;5;241m=\u001b[39mhigh, size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m4\u001b[39m,))\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m    194\u001b[0m     np\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    195\u001b[0m )\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ob(), {}\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/classic_control/acrobot.py:363\u001b[0m, in \u001b[0;36mAcrobotEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m     gfxdraw\u001b[38;5;241m.\u001b[39mfilled_circle(surf, \u001b[38;5;28mint\u001b[39m(x), \u001b[38;5;28mint\u001b[39m(y), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m scale), (\u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m204\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    362\u001b[0m surf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(surf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n",
      "\u001b[0;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "110c0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class PolicyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, action_space):\n",
    "        super(PolicyModel, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.action_space = action_space\n",
    "        self.model =nn.Sequential(\n",
    "            nn.Linear(input_dim, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, action_space),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8dd5da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyModel(6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b9c57218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.9991336 ,  0.04161806,  0.9985474 , -0.05388077, -0.03040618,\n",
      "        0.067844  ], dtype=float32), tensor(1), -1.0)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     14\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(torch\u001b[38;5;241m.\u001b[39mtensor(observation)))\n\u001b[0;32m---> 16\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     d\u001b[38;5;241m.\u001b[39mappend((observation, action, reward))\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/classic_control/acrobot.py:227\u001b[0m, in \u001b[0;36mAcrobotEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    224\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ob(), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {})\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/gymnasium/envs/classic_control/acrobot.py:367\u001b[0m, in \u001b[0;36mAcrobotEnv.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    366\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "episodes=100\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    d = []\n",
    "    \n",
    "    observation, info = env.reset()\n",
    "    \n",
    "    for i in range(100):\n",
    "        \n",
    "        action = torch.argmax(model(torch.tensor(observation)))\n",
    "        \n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        d.append((observation, action, reward))\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(d[0])\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: -500.0\n",
      "Episode 2/100, Total Reward: -500.0\n",
      "Episode 3/100, Total Reward: -500.0\n",
      "Episode 4/100, Total Reward: -500.0\n",
      "Episode 5/100, Total Reward: -500.0\n",
      "Episode 6/100, Total Reward: -500.0\n",
      "Episode 7/100, Total Reward: -500.0\n",
      "Episode 8/100, Total Reward: -500.0\n",
      "Episode 9/100, Total Reward: -500.0\n",
      "Episode 10/100, Total Reward: -500.0\n",
      "Episode 11/100, Total Reward: -500.0\n",
      "Episode 12/100, Total Reward: -500.0\n",
      "Episode 13/100, Total Reward: -500.0\n",
      "Episode 14/100, Total Reward: -500.0\n",
      "Episode 15/100, Total Reward: -500.0\n",
      "Episode 16/100, Total Reward: -500.0\n",
      "Episode 17/100, Total Reward: -500.0\n",
      "Episode 18/100, Total Reward: -500.0\n",
      "Episode 19/100, Total Reward: -500.0\n",
      "Episode 20/100, Total Reward: -500.0\n",
      "Episode 21/100, Total Reward: -500.0\n",
      "Episode 22/100, Total Reward: -500.0\n",
      "Episode 23/100, Total Reward: -500.0\n",
      "Episode 24/100, Total Reward: -500.0\n",
      "Episode 25/100, Total Reward: -500.0\n",
      "Episode 26/100, Total Reward: -500.0\n",
      "Episode 27/100, Total Reward: -500.0\n",
      "Episode 28/100, Total Reward: -500.0\n",
      "Episode 29/100, Total Reward: -500.0\n",
      "Episode 30/100, Total Reward: -500.0\n",
      "Episode 31/100, Total Reward: -500.0\n",
      "Episode 32/100, Total Reward: -500.0\n",
      "Episode 33/100, Total Reward: -500.0\n",
      "Episode 34/100, Total Reward: -500.0\n",
      "Episode 35/100, Total Reward: -500.0\n",
      "Episode 36/100, Total Reward: -500.0\n",
      "Episode 37/100, Total Reward: -500.0\n",
      "Episode 38/100, Total Reward: -500.0\n",
      "Episode 39/100, Total Reward: -500.0\n",
      "Episode 40/100, Total Reward: -500.0\n",
      "Episode 41/100, Total Reward: -500.0\n",
      "Episode 42/100, Total Reward: -500.0\n",
      "Episode 43/100, Total Reward: -500.0\n",
      "Episode 44/100, Total Reward: -500.0\n",
      "Episode 45/100, Total Reward: -500.0\n",
      "Episode 46/100, Total Reward: -500.0\n",
      "Episode 47/100, Total Reward: -500.0\n",
      "Episode 48/100, Total Reward: -500.0\n",
      "Episode 49/100, Total Reward: -500.0\n",
      "Episode 50/100, Total Reward: -500.0\n",
      "Episode 51/100, Total Reward: -500.0\n",
      "Episode 52/100, Total Reward: -500.0\n",
      "Episode 53/100, Total Reward: -500.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assume 'model' is your policy network\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "episodes = 100\n",
    "gamma = 0.99  # discount factor\n",
    "\n",
    "for e in range(episodes):\n",
    "    episode_data = []\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    \n",
    "    # Generate an episode\n",
    "    for t in range(1000):  # 100 is an arbitrary choice for max episode length\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32)\n",
    "        action = torch.argmax(model(state_tensor)).item()\n",
    "#         action = torch.multinomial(action_probs, 1).item()\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_data.append((state, action, reward))\n",
    "        \n",
    "        state = next_state\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # Calculate policy gradient and update model\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for s, a, r in reversed(episode_data):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-7)  # normalize\n",
    "\n",
    "    \n",
    "    # Calculate policy gradient and update model\n",
    "    policy_loss = []\n",
    "    for (s, a, r), R in zip(episode_data, returns):\n",
    "        state_tensor = torch.tensor(s, dtype=torch.float32)\n",
    "        action_probs = model(state_tensor)\n",
    "        action_prob = action_probs[a]\n",
    "        # Using unsqueeze to add a dimension\n",
    "        policy_loss.append(-torch.log(action_prob) * R)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Concatenating the policy loss list\n",
    "    policy_loss = torch.cat([loss.unsqueeze(0) for loss in policy_loss]).sum()\n",
    "\n",
    "\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Episode {e+1}/{episodes}, Total Reward: {sum([x[2] for x in episode_data])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e8ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bb79b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
